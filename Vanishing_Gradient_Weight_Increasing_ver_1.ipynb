{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Vanishing_Gradient_Weight_Increasing_ver_1 ",
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "2tcqqHL3-nSe"
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision.datasets import FashionMNIST\n",
        "import os\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "trusted": true,
        "id": "DalQIhJu-nSf"
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "SEED = 42\n",
        "set_seed(SEED)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Bzlfod7f-nSf"
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = '/kaggle/working/'\n",
        "train_dataset = FashionMNIST(os.path.join(root_dir, './data'), train= True, download = True, transform = transforms.ToTensor())\n",
        "test_dataset = FashionMNIST(os.path.join(root_dir, './data'), train= False, download = True, transform = transforms.ToTensor())"
      ],
      "metadata": {
        "trusted": true,
        "id": "ddHOwkq5-nSg",
        "outputId": "95201c4c-bba5-4c2b-94dd-75497da120d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /kaggle/working/./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 14.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /kaggle/working/./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to /kaggle/working/./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /kaggle/working/./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 209kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /kaggle/working/./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /kaggle/working/./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /kaggle/working/./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.94MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /kaggle/working/./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /kaggle/working/./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /kaggle/working/./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 15.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /kaggle/working/./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /kaggle/working/./data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 512\n",
        "train_ratio = 0.9\n",
        "\n",
        "train_size = int(len(train_dataset) * 0.9)\n",
        "val_size = len(train_dataset) - train_size\n",
        "\n",
        "train_subset, val_subset = random_split(train_dataset,[train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_subset, batch_size = batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_subset, batch_size = batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Train size: {len(train_subset)}\")\n",
        "print(f\"Validation size: {len(val_subset)}\")\n",
        "print(f\"Test size: {len(test_dataset)}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "Kxd8vl_V-nSg",
        "outputId": "5812e204-fb4f-4c48-dc9d-da2fc5ed6a45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 54000\n",
            "Validation size: 6000\n",
            "Test size: 10000\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dims, hidden_dims, output_dims):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dims, hidden_dims)\n",
        "        self.layer2 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.layer3 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.layer4 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.layer5 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.layer6 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.layer7 = nn.Linear(hidden_dims, hidden_dims)\n",
        "        self.output = nn.Linear(hidden_dims, output_dims)\n",
        "\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.normal_(module.weight, mean=0.0, std =1.0)\n",
        "                nn.init.constant_(module.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.Flatten()(x)\n",
        "        x = self.layer1(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        x = self.layer2(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        x = self.layer3(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        x = self.layer4(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        x = self.layer5(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        x = self.layer6(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        x = self.layer7(x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "        out = self.output(x)\n",
        "        return out\n",
        "\n",
        "def compute_accuracy(preds, labels):\n",
        "    acc = (torch.argmax(preds, dim = 1) == labels).sum()\n",
        "    return acc"
      ],
      "metadata": {
        "trusted": true,
        "id": "Z_GotUh6-nSg"
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "input_dims = 784\n",
        "hidden_dims = 128\n",
        "output_dims = 10\n",
        "lr = 1e-2\n",
        "\n",
        "model = MLP(input_dims=input_dims, hidden_dims=hidden_dims, output_dims=output_dims).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Na812cyU-nSh"
      },
      "outputs": [],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "train_loss_lst = []\n",
        "train_acc_lst = []\n",
        "val_loss_lst = []\n",
        "val_acc_lst = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    count = 0\n",
        "    model.train()\n",
        "    for X_train, y_train in train_loader:\n",
        "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = model.forward(X_train)\n",
        "        loss = criterion(preds, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss\n",
        "        train_acc += compute_accuracy(preds, y_train)\n",
        "        count += len(y_train)\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_loss_lst.append(train_loss.detach().cpu().numpy())\n",
        "    train_acc /= count\n",
        "    train_acc_lst.append(train_acc.detach().cpu().numpy())\n",
        "\n",
        "\n",
        "\n",
        "    val_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    count = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for X_val, y_val in val_loader:\n",
        "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
        "            preds = model.forward(X_val)\n",
        "            loss = criterion(preds, y_val)\n",
        "            val_loss += loss\n",
        "            val_acc += compute_accuracy(preds, y_val)\n",
        "            count += len(y_val)\n",
        "\n",
        "    val_loss /= len(test_loader)\n",
        "    val_loss_lst.append(val_loss.detach().cpu().numpy())\n",
        "    val_acc /= count\n",
        "    val_acc_lst.append(val_acc.detach().cpu().numpy())\n",
        "\n",
        "    print(f\"EPOCH {epoch + 1}/{epochs}, Train_Loss: {train_loss:.4f}, Train_Acc: {train_acc:.4f}, Validation Loss: {val_loss:.4f}, Val_Acc: {val_acc:.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "51bYCwFW-nSh",
        "outputId": "c1cac1a7-9b03-4bd3-f80c-b0cb23345e83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1/100, Train_Loss: 4.1082, Train_Acc: 0.1834, Validation Loss: 1.3233, Val_Acc: 0.2307\n",
            "EPOCH 2/100, Train_Loss: 2.0284, Train_Acc: 0.2856, Validation Loss: 1.1436, Val_Acc: 0.3317\n",
            "EPOCH 3/100, Train_Loss: 1.7839, Train_Acc: 0.3702, Validation Loss: 1.0232, Val_Acc: 0.3933\n",
            "EPOCH 4/100, Train_Loss: 1.6172, Train_Acc: 0.4251, Validation Loss: 0.9416, Val_Acc: 0.4423\n",
            "EPOCH 5/100, Train_Loss: 1.5005, Train_Acc: 0.4642, Validation Loss: 0.8810, Val_Acc: 0.4765\n",
            "EPOCH 6/100, Train_Loss: 1.4126, Train_Acc: 0.4928, Validation Loss: 0.8357, Val_Acc: 0.5013\n",
            "EPOCH 7/100, Train_Loss: 1.3454, Train_Acc: 0.5169, Validation Loss: 0.7984, Val_Acc: 0.5225\n",
            "EPOCH 8/100, Train_Loss: 1.2895, Train_Acc: 0.5355, Validation Loss: 0.7687, Val_Acc: 0.5427\n",
            "EPOCH 9/100, Train_Loss: 1.2430, Train_Acc: 0.5530, Validation Loss: 0.7436, Val_Acc: 0.5587\n",
            "EPOCH 10/100, Train_Loss: 1.2037, Train_Acc: 0.5673, Validation Loss: 0.7210, Val_Acc: 0.5723\n",
            "EPOCH 11/100, Train_Loss: 1.1693, Train_Acc: 0.5802, Validation Loss: 0.7021, Val_Acc: 0.5838\n",
            "EPOCH 12/100, Train_Loss: 1.1389, Train_Acc: 0.5914, Validation Loss: 0.6849, Val_Acc: 0.5918\n",
            "EPOCH 13/100, Train_Loss: 1.1133, Train_Acc: 0.5995, Validation Loss: 0.6706, Val_Acc: 0.5968\n",
            "EPOCH 14/100, Train_Loss: 1.0892, Train_Acc: 0.6076, Validation Loss: 0.6572, Val_Acc: 0.6073\n",
            "EPOCH 15/100, Train_Loss: 1.0682, Train_Acc: 0.6145, Validation Loss: 0.6454, Val_Acc: 0.6122\n",
            "EPOCH 16/100, Train_Loss: 1.0491, Train_Acc: 0.6214, Validation Loss: 0.6344, Val_Acc: 0.6173\n",
            "EPOCH 17/100, Train_Loss: 1.0306, Train_Acc: 0.6270, Validation Loss: 0.6237, Val_Acc: 0.6225\n",
            "EPOCH 18/100, Train_Loss: 1.0143, Train_Acc: 0.6329, Validation Loss: 0.6147, Val_Acc: 0.6277\n",
            "EPOCH 19/100, Train_Loss: 0.9991, Train_Acc: 0.6376, Validation Loss: 0.6065, Val_Acc: 0.6325\n",
            "EPOCH 20/100, Train_Loss: 0.9854, Train_Acc: 0.6427, Validation Loss: 0.5976, Val_Acc: 0.6373\n",
            "EPOCH 21/100, Train_Loss: 0.9719, Train_Acc: 0.6462, Validation Loss: 0.5908, Val_Acc: 0.6413\n",
            "EPOCH 22/100, Train_Loss: 0.9592, Train_Acc: 0.6507, Validation Loss: 0.5840, Val_Acc: 0.6443\n",
            "EPOCH 23/100, Train_Loss: 0.9476, Train_Acc: 0.6541, Validation Loss: 0.5773, Val_Acc: 0.6468\n",
            "EPOCH 24/100, Train_Loss: 0.9367, Train_Acc: 0.6578, Validation Loss: 0.5706, Val_Acc: 0.6502\n",
            "EPOCH 25/100, Train_Loss: 0.9260, Train_Acc: 0.6616, Validation Loss: 0.5653, Val_Acc: 0.6543\n",
            "EPOCH 26/100, Train_Loss: 0.9174, Train_Acc: 0.6650, Validation Loss: 0.5600, Val_Acc: 0.6583\n",
            "EPOCH 27/100, Train_Loss: 0.9076, Train_Acc: 0.6679, Validation Loss: 0.5548, Val_Acc: 0.6637\n",
            "EPOCH 28/100, Train_Loss: 0.8988, Train_Acc: 0.6708, Validation Loss: 0.5504, Val_Acc: 0.6633\n",
            "EPOCH 29/100, Train_Loss: 0.8913, Train_Acc: 0.6732, Validation Loss: 0.5456, Val_Acc: 0.6663\n",
            "EPOCH 30/100, Train_Loss: 0.8826, Train_Acc: 0.6760, Validation Loss: 0.5414, Val_Acc: 0.6690\n",
            "EPOCH 31/100, Train_Loss: 0.8760, Train_Acc: 0.6787, Validation Loss: 0.5374, Val_Acc: 0.6717\n",
            "EPOCH 32/100, Train_Loss: 0.8694, Train_Acc: 0.6806, Validation Loss: 0.5341, Val_Acc: 0.6760\n",
            "EPOCH 33/100, Train_Loss: 0.8627, Train_Acc: 0.6829, Validation Loss: 0.5303, Val_Acc: 0.6757\n",
            "EPOCH 34/100, Train_Loss: 0.8563, Train_Acc: 0.6857, Validation Loss: 0.5262, Val_Acc: 0.6812\n",
            "EPOCH 35/100, Train_Loss: 0.8497, Train_Acc: 0.6875, Validation Loss: 0.5233, Val_Acc: 0.6802\n",
            "EPOCH 36/100, Train_Loss: 0.8440, Train_Acc: 0.6895, Validation Loss: 0.5199, Val_Acc: 0.6842\n",
            "EPOCH 37/100, Train_Loss: 0.8386, Train_Acc: 0.6918, Validation Loss: 0.5171, Val_Acc: 0.6860\n",
            "EPOCH 38/100, Train_Loss: 0.8335, Train_Acc: 0.6929, Validation Loss: 0.5141, Val_Acc: 0.6878\n",
            "EPOCH 39/100, Train_Loss: 0.8285, Train_Acc: 0.6946, Validation Loss: 0.5111, Val_Acc: 0.6895\n",
            "EPOCH 40/100, Train_Loss: 0.8233, Train_Acc: 0.6968, Validation Loss: 0.5082, Val_Acc: 0.6900\n",
            "EPOCH 41/100, Train_Loss: 0.8185, Train_Acc: 0.6986, Validation Loss: 0.5055, Val_Acc: 0.6928\n",
            "EPOCH 42/100, Train_Loss: 0.8142, Train_Acc: 0.6996, Validation Loss: 0.5038, Val_Acc: 0.6923\n",
            "EPOCH 43/100, Train_Loss: 0.8097, Train_Acc: 0.7012, Validation Loss: 0.5017, Val_Acc: 0.6940\n",
            "EPOCH 44/100, Train_Loss: 0.8061, Train_Acc: 0.7034, Validation Loss: 0.4985, Val_Acc: 0.6972\n",
            "EPOCH 45/100, Train_Loss: 0.8017, Train_Acc: 0.7039, Validation Loss: 0.4959, Val_Acc: 0.6982\n",
            "EPOCH 46/100, Train_Loss: 0.7975, Train_Acc: 0.7060, Validation Loss: 0.4941, Val_Acc: 0.7005\n",
            "EPOCH 47/100, Train_Loss: 0.7930, Train_Acc: 0.7071, Validation Loss: 0.4921, Val_Acc: 0.6988\n",
            "EPOCH 48/100, Train_Loss: 0.7902, Train_Acc: 0.7081, Validation Loss: 0.4903, Val_Acc: 0.7000\n",
            "EPOCH 49/100, Train_Loss: 0.7858, Train_Acc: 0.7091, Validation Loss: 0.4880, Val_Acc: 0.7013\n",
            "EPOCH 50/100, Train_Loss: 0.7823, Train_Acc: 0.7106, Validation Loss: 0.4864, Val_Acc: 0.7033\n",
            "EPOCH 51/100, Train_Loss: 0.7792, Train_Acc: 0.7112, Validation Loss: 0.4844, Val_Acc: 0.7043\n",
            "EPOCH 52/100, Train_Loss: 0.7753, Train_Acc: 0.7123, Validation Loss: 0.4828, Val_Acc: 0.7047\n",
            "EPOCH 53/100, Train_Loss: 0.7724, Train_Acc: 0.7143, Validation Loss: 0.4805, Val_Acc: 0.7055\n",
            "EPOCH 54/100, Train_Loss: 0.7686, Train_Acc: 0.7153, Validation Loss: 0.4793, Val_Acc: 0.7055\n",
            "EPOCH 55/100, Train_Loss: 0.7658, Train_Acc: 0.7167, Validation Loss: 0.4779, Val_Acc: 0.7088\n",
            "EPOCH 56/100, Train_Loss: 0.7623, Train_Acc: 0.7177, Validation Loss: 0.4766, Val_Acc: 0.7085\n",
            "EPOCH 57/100, Train_Loss: 0.7596, Train_Acc: 0.7185, Validation Loss: 0.4743, Val_Acc: 0.7103\n",
            "EPOCH 58/100, Train_Loss: 0.7567, Train_Acc: 0.7195, Validation Loss: 0.4731, Val_Acc: 0.7097\n",
            "EPOCH 59/100, Train_Loss: 0.7539, Train_Acc: 0.7204, Validation Loss: 0.4724, Val_Acc: 0.7097\n",
            "EPOCH 60/100, Train_Loss: 0.7514, Train_Acc: 0.7212, Validation Loss: 0.4702, Val_Acc: 0.7113\n",
            "EPOCH 61/100, Train_Loss: 0.7478, Train_Acc: 0.7229, Validation Loss: 0.4686, Val_Acc: 0.7130\n",
            "EPOCH 62/100, Train_Loss: 0.7453, Train_Acc: 0.7237, Validation Loss: 0.4674, Val_Acc: 0.7133\n",
            "EPOCH 63/100, Train_Loss: 0.7428, Train_Acc: 0.7250, Validation Loss: 0.4666, Val_Acc: 0.7138\n",
            "EPOCH 64/100, Train_Loss: 0.7396, Train_Acc: 0.7250, Validation Loss: 0.4650, Val_Acc: 0.7157\n",
            "EPOCH 65/100, Train_Loss: 0.7368, Train_Acc: 0.7275, Validation Loss: 0.4636, Val_Acc: 0.7153\n",
            "EPOCH 66/100, Train_Loss: 0.7353, Train_Acc: 0.7282, Validation Loss: 0.4625, Val_Acc: 0.7152\n",
            "EPOCH 67/100, Train_Loss: 0.7322, Train_Acc: 0.7284, Validation Loss: 0.4604, Val_Acc: 0.7172\n",
            "EPOCH 68/100, Train_Loss: 0.7301, Train_Acc: 0.7296, Validation Loss: 0.4598, Val_Acc: 0.7175\n",
            "EPOCH 69/100, Train_Loss: 0.7280, Train_Acc: 0.7302, Validation Loss: 0.4587, Val_Acc: 0.7162\n",
            "EPOCH 70/100, Train_Loss: 0.7253, Train_Acc: 0.7313, Validation Loss: 0.4574, Val_Acc: 0.7212\n",
            "EPOCH 71/100, Train_Loss: 0.7237, Train_Acc: 0.7323, Validation Loss: 0.4566, Val_Acc: 0.7215\n",
            "EPOCH 72/100, Train_Loss: 0.7209, Train_Acc: 0.7327, Validation Loss: 0.4549, Val_Acc: 0.7197\n",
            "EPOCH 73/100, Train_Loss: 0.7187, Train_Acc: 0.7333, Validation Loss: 0.4539, Val_Acc: 0.7207\n",
            "EPOCH 74/100, Train_Loss: 0.7170, Train_Acc: 0.7348, Validation Loss: 0.4524, Val_Acc: 0.7207\n",
            "EPOCH 75/100, Train_Loss: 0.7154, Train_Acc: 0.7348, Validation Loss: 0.4517, Val_Acc: 0.7217\n",
            "EPOCH 76/100, Train_Loss: 0.7127, Train_Acc: 0.7360, Validation Loss: 0.4500, Val_Acc: 0.7230\n",
            "EPOCH 77/100, Train_Loss: 0.7101, Train_Acc: 0.7372, Validation Loss: 0.4494, Val_Acc: 0.7233\n",
            "EPOCH 78/100, Train_Loss: 0.7081, Train_Acc: 0.7377, Validation Loss: 0.4490, Val_Acc: 0.7228\n",
            "EPOCH 79/100, Train_Loss: 0.7065, Train_Acc: 0.7387, Validation Loss: 0.4473, Val_Acc: 0.7243\n",
            "EPOCH 80/100, Train_Loss: 0.7048, Train_Acc: 0.7393, Validation Loss: 0.4470, Val_Acc: 0.7253\n",
            "EPOCH 81/100, Train_Loss: 0.7030, Train_Acc: 0.7405, Validation Loss: 0.4461, Val_Acc: 0.7265\n",
            "EPOCH 82/100, Train_Loss: 0.7007, Train_Acc: 0.7407, Validation Loss: 0.4449, Val_Acc: 0.7265\n",
            "EPOCH 83/100, Train_Loss: 0.6989, Train_Acc: 0.7418, Validation Loss: 0.4438, Val_Acc: 0.7265\n",
            "EPOCH 84/100, Train_Loss: 0.6972, Train_Acc: 0.7423, Validation Loss: 0.4429, Val_Acc: 0.7268\n",
            "EPOCH 85/100, Train_Loss: 0.6953, Train_Acc: 0.7426, Validation Loss: 0.4418, Val_Acc: 0.7290\n",
            "EPOCH 86/100, Train_Loss: 0.6937, Train_Acc: 0.7439, Validation Loss: 0.4414, Val_Acc: 0.7280\n",
            "EPOCH 87/100, Train_Loss: 0.6917, Train_Acc: 0.7438, Validation Loss: 0.4398, Val_Acc: 0.7290\n",
            "EPOCH 88/100, Train_Loss: 0.6904, Train_Acc: 0.7449, Validation Loss: 0.4392, Val_Acc: 0.7295\n",
            "EPOCH 89/100, Train_Loss: 0.6883, Train_Acc: 0.7457, Validation Loss: 0.4385, Val_Acc: 0.7313\n",
            "EPOCH 90/100, Train_Loss: 0.6871, Train_Acc: 0.7469, Validation Loss: 0.4379, Val_Acc: 0.7332\n",
            "EPOCH 91/100, Train_Loss: 0.6857, Train_Acc: 0.7467, Validation Loss: 0.4368, Val_Acc: 0.7318\n",
            "EPOCH 92/100, Train_Loss: 0.6837, Train_Acc: 0.7475, Validation Loss: 0.4365, Val_Acc: 0.7327\n",
            "EPOCH 93/100, Train_Loss: 0.6823, Train_Acc: 0.7478, Validation Loss: 0.4352, Val_Acc: 0.7330\n",
            "EPOCH 94/100, Train_Loss: 0.6809, Train_Acc: 0.7491, Validation Loss: 0.4347, Val_Acc: 0.7320\n",
            "EPOCH 95/100, Train_Loss: 0.6798, Train_Acc: 0.7486, Validation Loss: 0.4336, Val_Acc: 0.7343\n",
            "EPOCH 96/100, Train_Loss: 0.6779, Train_Acc: 0.7498, Validation Loss: 0.4332, Val_Acc: 0.7343\n",
            "EPOCH 97/100, Train_Loss: 0.6760, Train_Acc: 0.7503, Validation Loss: 0.4323, Val_Acc: 0.7348\n",
            "EPOCH 98/100, Train_Loss: 0.6751, Train_Acc: 0.7510, Validation Loss: 0.4321, Val_Acc: 0.7365\n",
            "EPOCH 99/100, Train_Loss: 0.6734, Train_Acc: 0.7516, Validation Loss: 0.4329, Val_Acc: 0.7362\n",
            "EPOCH 100/100, Train_Loss: 0.6715, Train_Acc: 0.7517, Validation Loss: 0.4302, Val_Acc: 0.7373\n"
          ]
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(2, 2, figsize=(12, 10))\n",
        "ax[0, 0].plot(train_loss_lst, color='green')\n",
        "ax[0, 0].set(xlabel='Epoch', ylabel='Loss')\n",
        "ax[0, 0].set_title('Training Loss')\n",
        "\n",
        "ax[0, 1].plot(val_loss_lst, color='orange')\n",
        "ax[0, 1].set(xlabel='Epoch', ylabel='Loss')\n",
        "ax[0, 1].set_title('Validation Loss')\n",
        "\n",
        "ax[1, 0].plot(train_acc_lst, color='green')\n",
        "ax[1, 0].set(xlabel='Epoch', ylabel='Accuracy')\n",
        "ax[1, 0].set_title('Training Accuracy')\n",
        "\n",
        "ax[1, 1].plot(val_acc_lst, color='orange')\n",
        "ax[1, 1].set(xlabel='Epoch', ylabel='Accuracy')\n",
        "ax[1, 1].set_title('Validation Accuracy')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "8-cSmH7M-nSh",
        "outputId": "3d86773c-1fc0-457d-9310-78115a805966",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-075b201615b4>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss_lst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'green'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mylabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1719\u001b[0m         \"\"\"\n\u001b[1;32m   1720\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1721\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1722\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             yield from self._plot_args(\n\u001b[0m\u001b[1;32m    304\u001b[0m                 axes, this, kwargs, ambiguous_fmt_datakey=ambiguous_fmt_datakey)\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/cbook.py\u001b[0m in \u001b[0;36mindex_of\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m   1664\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1665\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1666\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1667\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mVisibleDeprecationWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m         \u001b[0;31m# NumPy 1.19 will warn on ragged input, and we can't actually use it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/cbook.py\u001b[0m in \u001b[0;36m_check_1d\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m             len(x.shape) < 1):\n\u001b[0;32m-> 1358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1359\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1000 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": 11
    },
    {
      "cell_type": "code",
      "source": [
        "val_target = []\n",
        "val_predict = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for X_val, y_val in val_loader:\n",
        "        X_val = X_val.to(device)\n",
        "        y_val = y_val.to(device)\n",
        "        outputs = model(X_val)\n",
        "\n",
        "        val_predict.append(outputs.cpu())\n",
        "        val_target.append(y_val.cpu())\n",
        "\n",
        "    val_predict = torch.cat(val_predict)\n",
        "    val_target = torch.cat(val_target)\n",
        "    val_acc = (torch.argmax(val_predict, 1) == val_target).sum().item() / len(val_target)\n",
        "\n",
        "    print('Evaluation on val set:')\n",
        "    print(f'Accuracy: {val_acc}')"
      ],
      "metadata": {
        "trusted": true,
        "id": "fRnQaE20-nSi"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}